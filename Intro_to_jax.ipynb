{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install JAX and Flax"
      ],
      "metadata": {
        "id": "M4BDgO32pGCj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9NPylF8oi_Y",
        "outputId": "923f70ed-998d-410a-f1ab-c19886ad827b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -q flax optax transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to JAX for ML"
      ],
      "metadata": {
        "id": "N8fOq_BJpgAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Import Required Libraries"
      ],
      "metadata": {
        "id": "Rtf9juhmpsc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "-1unDUOepX5E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Check Available Devices"
      ],
      "metadata": {
        "id": "e2yS1NlisGCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.devices()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpYcoRd9sNDU",
        "outputId": "dc979aee-76b0-4bfb-cf79-354a6c866aac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3️⃣ Implement a Simple Function and Compute the Gradient"
      ],
      "metadata": {
        "id": "7TLLW_i1sMlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(x):\n",
        "    return x**2 + 3*x + 2  # Quadratic function\n",
        "\n",
        "grad_loss = grad(loss_fn)  # Compute derivative\n",
        "\n",
        "x = jnp.array(5.0)\n",
        "print(\"Loss:\", loss_fn(x))\n",
        "print(\"Gradient at x=5:\", grad_loss(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAEqSVDusVL4",
        "outputId": "994dd739-31ba-4c4c-e21b-f6d2a9d0154b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 42.0\n",
            "Gradient at x=5: 13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation for grad (Automatic Differentiation in JAX)\n",
        "\n",
        "What is grad?\n",
        "grad is a function in JAX that computes the gradient (derivative) of a function with respect to its input.\n",
        "It is useful for training neural networks and optimizing functions in machine learning.\n",
        "\n",
        "Explanation:\n",
        "The function loss_fn(x) = x² + 3x + 2 is differentiable.\n",
        "grad(loss_fn) returns another function that computes ∂(loss_fn)/∂x.\n",
        "The gradient at x = 5 is calculated using automatic differentiation."
      ],
      "metadata": {
        "id": "u3ClmBPhti4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4️⃣ Use jit for Faster Execution"
      ],
      "metadata": {
        "id": "idZzc8NIshzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def compute_square(x):\n",
        "    return x ** 2\n",
        "\n",
        "x = jnp.array([2.0, 3.0, 4.0])\n",
        "print(compute_square(x))  # JIT-optimized execution\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r595Hq4Ksi6A",
        "outputId": "f51b1b86-38a1-438d-ba6f-32f436b92530"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4.  9. 16.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation for jit (Just-In-Time Compilation in JAX)\n",
        "\n",
        "What is jit?\n",
        "jit compiles your Python functions into highly optimized machine code.\n",
        "It speeds up numerical computations, especially in deep learning.\n",
        "\n",
        "Explanation:\n",
        "The function compute_square(x) = x² is decorated with @jit, meaning JAX will compile it using XLA (Accelerated Linear Algebra).\n",
        "This makes execution much faster, especially on GPUs/TPUs.\n",
        "JIT Compilation is useful for training large models efficiently."
      ],
      "metadata": {
        "id": "OVWbmk3Ot1Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX’s NumPy Replacement"
      ],
      "metadata": {
        "id": "NiSXpQ6Bu3sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "x = jnp.array([1.0, 2.0, 3.0])\n",
        "y = jnp.sin(x)  # Compute sin(x) using jax.numpy\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAA4fSCbvTKj",
        "outputId": "7e3e27f6-f5c9-436e-d133-fb0010c53e36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.84147096 0.9092974  0.14112003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is jax.numpy?\n",
        "jax.numpy (jnp) is a drop-in replacement for NumPy, optimized for automatic differentiation (grad) and GPU/TPU acceleration.\n",
        "Functions in jax.numpy behave like NumPy but can be JIT-compiled and auto-differentiated.\n"
      ],
      "metadata": {
        "id": "sGR1t0MVvWBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Use jax.numpy Instead of NumPy?\n",
        "Supports GPU & TPU acceleration automatically.\n",
        "Works seamlessly with grad and jit for machine learning applications.\n",
        "Allows vectorized computations for faster performance."
      ],
      "metadata": {
        "id": "x_kh-MbpvZks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WG5FgHxpviHl"
      }
    }
  ]
}